{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "This lab is designed to help you solidify your understanding of embeddings by applying them to tasks like semantic similarity, clustering, and building a semantic search system.\n",
    "\n",
    "### Tasks:\n",
    "- Task 1: Semantic Similarity Comparison\n",
    "- Task 2: Document Clustering\n",
    "- Task 3: Enhance the Semantic Search System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Semantic Similarity Comparison\n",
    "### Objective:\n",
    "Compare semantic similarity between pairs of sentences using cosine similarity and embeddings.\n",
    "\n",
    "### Steps:\n",
    "1. Load a pre-trained Sentence Transformer model.\n",
    "2. Encode the sentence pairs.\n",
    "3. Compute cosine similarity for each pair.\n",
    "\n",
    "### Dataset:\n",
    "- \"A dog is playing in the park.\" vs. \"A dog is running in a field.\"\n",
    "- \"I love pizza.\" vs. \"I enjoy ice cream.\"\n",
    "- \"What is AI?\" vs. \"How does a computer learn?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"DISABLE_TQDM\"] = \"1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   sentence_1                   sentence_2  cosine_similarity\n",
      "                I love pizza.           I enjoy ice cream.           0.528068\n",
      "A dog is playing in the park. A dog is running in a field.           0.521975\n",
      "                  What is AI?   How does a computer learn?           0.319435\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sentence pairs\n",
    "sentence_pairs = [\n",
    "    (\"A dog is playing in the park.\", \"A dog is running in a field.\"),\n",
    "    (\"I love pizza.\", \"I enjoy ice cream.\"),\n",
    "    (\"What is AI?\", \"How does a computer learn?\")\n",
    "]\n",
    "\n",
    "# Compute similarities into a neat table\n",
    "rows = []\n",
    "for s1, s2 in sentence_pairs:\n",
    "    e1 = model.encode(s1, normalize_embeddings=True)  # no progress bars here\n",
    "    e2 = model.encode(s2, normalize_embeddings=True)\n",
    "    sim = float(cosine_similarity([e1], [e2])[0][0])\n",
    "    rows.append({\"sentence_1\": s1, \"sentence_2\": s2, \"cosine_similarity\": sim})\n",
    "\n",
    "sim_df = pd.DataFrame(rows).sort_values(\"cosine_similarity\", ascending=False)\n",
    "print(sim_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Which sentence pairs are the most semantically similar? Why? - “I love pizza.” vs. “I enjoy ice cream.” (cosine similarity ≈ 0.53). The pair involves paraphrase-like relationships. “love” and “enjoy” are semantically close, and “pizza” and “ice cream” are both food items.\n",
    "- Can you think of cases where cosine similarity might fail to capture true semantic meaning? Antonyms, context, sacarsm, numeric differences ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Document Clustering\n",
    "### Objective:\n",
    "Cluster a set of text documents into similar groups based on their embeddings.\n",
    "\n",
    "### Steps:\n",
    "1. Encode the documents using Sentence Transformers.\n",
    "2. Use KMeans clustering to group the documents.\n",
    "3. Analyze the clusters for semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"sentence-transformers\"])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments:\n",
      "                                          doc  cluster\n",
      "       How do I change a flat tire on a car?        0\n",
      "                How do I fix a leaky faucet?        0\n",
      "              What is the capital of France?        1\n",
      "             How do I bake a chocolate cake?        1\n",
      "What is the distance between Earth and Mars?        1\n",
      "       What is the best way to learn Python?        2\n",
      "\n",
      "Representative document per cluster:\n",
      "  Cluster 0: How do I change a flat tire on a car?\n",
      "  Cluster 1: What is the distance between Earth and Mars?\n",
      "  Cluster 2: What is the best way to learn Python?\n"
     ]
    }
   ],
   "source": [
    "# Documents to cluster\n",
    "documents = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the distance between Earth and Mars?\",\n",
    "    \"How do I change a flat tire on a car?\",\n",
    "    \"What is the best way to learn Python?\",\n",
    "    \"How do I fix a leaky faucet?\"\n",
    "]\n",
    "\n",
    "# Encode documents\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(documents, normalize_embeddings=True)  # (n_docs, dim)\n",
    "\n",
    "#YOUR CODE HERE\n",
    "cand_ks = range(2, min(5, len(documents)-1)+1)  # 2..5 (bounded by data size)\n",
    "best_k, best_score = None, -1\n",
    "for k in cand_ks:\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    labels = km.fit_predict(embeddings)\n",
    "    # Silhouette needs >1 label; guard just in case\n",
    "    if len(set(labels)) > 1:\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        if score > best_score:\n",
    "            best_k, best_score = k, score\n",
    "if best_k is None:\n",
    "    best_k = 3\n",
    "\n",
    "# Final KMeans with best_k\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Pack results\n",
    "df = pd.DataFrame({\"doc\": documents, \"cluster\": labels})\n",
    "print(\"Cluster assignments:\\n\", df.sort_values(\"cluster\").to_string(index=False))\n",
    "\n",
    "# Representative doc per cluster \n",
    "closest_idx, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, embeddings)\n",
    "print(\"\\nRepresentative document per cluster:\")\n",
    "for c, idx in enumerate(closest_idx):\n",
    "    print(f\"  Cluster {c}: {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments:\n",
      "                                          doc  cluster\n",
      "       How do I change a flat tire on a car?        0\n",
      "                How do I fix a leaky faucet?        0\n",
      "              What is the capital of France?        1\n",
      "             How do I bake a chocolate cake?        1\n",
      "What is the distance between Earth and Mars?        1\n",
      "       What is the best way to learn Python?        2\n",
      "\n",
      "Representative document per cluster:\n",
      "  Cluster 0: How do I change a flat tire on a car?\n",
      "  Cluster 1: What is the distance between Earth and Mars?\n",
      "  Cluster 2: What is the best way to learn Python?\n"
     ]
    }
   ],
   "source": [
    "# Perform KMeans clustering\n",
    "\n",
    "# Choose number of clusters \n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "\n",
    "# Fit on embeddings and get labels\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Show assignments\n",
    "cluster_df = pd.DataFrame({\"doc\": documents, \"cluster\": labels})\n",
    "print(\"Cluster assignments:\\n\", cluster_df.sort_values(\"cluster\").to_string(index=False))\n",
    "\n",
    "# Show a representative document (closest to centroid) for each cluster\n",
    "closest_idx, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, embeddings)\n",
    "print(\"\\nRepresentative document per cluster:\")\n",
    "for c, idx in enumerate(closest_idx):\n",
    "    print(f\"  Cluster {c}: {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cluster 1] What is the capital of France?\n",
      "[Cluster 1] How do I bake a chocolate cake?\n",
      "[Cluster 1] What is the distance between Earth and Mars?\n",
      "[Cluster 0] How do I change a flat tire on a car?\n",
      "[Cluster 2] What is the best way to learn Python?\n",
      "[Cluster 0] How do I fix a leaky faucet?\n"
     ]
    }
   ],
   "source": [
    "# Print cluster assignments\n",
    "\n",
    "for doc, label in zip(documents, labels):\n",
    "    print(f\"[Cluster {label}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many clusters make the most sense? Why? The model found that three clusters (k = 3) make the most sense. With six questions, the embeddings naturally separate into about three semantic groups that share related meanings. This is also supported by the silhouette score, which was highest at k = 3, indicating good cohesion within clusters and clear separation between them.\n",
    "- Examine the documents in each cluster. Are they semantically meaningful? Can you assign a semantic \"theme\" to each cluster? Yes — the clusters are semantically meaningful and easy.\n",
    "Cluster 0 - Practical how-to tasks / household repairs\n",
    "Cluster 1 - General knowledge / informational questions\n",
    "Cluster 2 - Learning & technology\n",
    "- Try this exercise with a larger dataset of your choice\n",
    "Repeating this experiment with a larger dataset would make the clustering more robust. This would demonstrate how sentence embeddings capture semantic similarity and how KMeans can uncover latent themes within text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Semantic Search System\n",
    "### Objective:\n",
    "Create a semantic search engine:\n",
    "A user provides a query and you search the dataset for semantically relevant documents to return. Return the top 5 results.\n",
    "\n",
    "### Dataset:\n",
    "- Use the following set of documents:\n",
    "    - \"What is the capital of France?\"\n",
    "    - \"How do I bake a chocolate cake?\"\n",
    "    - \"What is the distance between Earth and Mars?\"\n",
    "    - \"How do I change a flat tire on a car?\"\n",
    "    - \"What is the best way to learn Python?\"\n",
    "    - \"How do I fix a leaky faucet?\"\n",
    "    - \"What are the best travel destinations in Europe?\"\n",
    "    - \"How do I set up a local server?\"\n",
    "    - \"What is quantum computing?\"\n",
    "    - \"How do I build a mobile app?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Documents dataset\n",
    "documents = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the distance between Earth and Mars?\",\n",
    "    \"How do I change a flat tire on a car?\",\n",
    "    \"What is the best way to learn Python?\",\n",
    "    \"How do I fix a leaky faucet?\",\n",
    "    \"What are the best travel destinations in Europe?\",\n",
    "    \"How do I set up a local server?\",\n",
    "    \"What is quantum computing?\",\n",
    "    \"How do I build a mobile app?\"\n",
    "]\n",
    "\n",
    "# Compute document embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "#YOUR CODE HERE\n",
    "doc_embeddings = model.encode(\n",
    "    documents,\n",
    "    normalize_embeddings=True,      # normalize for cosine similarity\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "print(\"Embeddings shape:\", doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def semantic_search(query, documents, doc_embeddings, top_n=5):\n",
    "\n",
    "    # Encode the query into an embedding\n",
    "    query_emb = model.encode(\n",
    "        [query],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "\n",
    "    # Compute cosine similarity between the query and all document embeddings\n",
    "    sims = cosine_similarity(query_emb, doc_embeddings)[0]\n",
    "\n",
    "    # Sort documents by similarity score (descending)\n",
    "    top_indices = np.argsort(sims)[::-1][:top_n]\n",
    "\n",
    "    # Prepare results\n",
    "    results = [(documents[i], float(sims[i])) for i in top_indices]\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is quantum computing?', 0.4352477192878723),\n",
       " ('What is the best way to learn Python?', 0.31878265738487244),\n",
       " ('How do I build a mobile app?', 0.11044073104858398),\n",
       " ('How do I set up a local server?', 0.09112647920846939),\n",
       " ('What are the best travel destinations in Europe?', 0.09064781665802002)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the search function\n",
    "query = \"Explain programming languages.\"\n",
    "semantic_search(query, documents, doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What are the top-ranked results for the given queries?\n",
    "1. What is quantum computing? — 0.435\n",
    "2. What is the best way to learn Python? — 0.319\n",
    "3. How do I build a mobile app? — 0.110\n",
    "4. How do I set up a local server? — 0.091\n",
    "5. What are the best travel destinations in Europe? — 0.091\n",
    "- How can you improve the ranking explanation for users? Display cosine similarity (rounded) and a “relevance” label: High (≥0.45), Medium (0.25–0.45), Low (<0.25). Extract and show overlapping concepts between query and doc. Include a short rationale line. Re-rank for better quality. Add a snippet. Hide items below a similarity threshold to avoid obviously off-topic results.\n",
    "- Try this approach with a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>relevance</th>\n",
       "      <th>document</th>\n",
       "      <th>why</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.435</td>\n",
       "      <td>Medium</td>\n",
       "      <td>What is quantum computing?</td>\n",
       "      <td>Shares CS/tech topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.319</td>\n",
       "      <td>Medium</td>\n",
       "      <td>What is the best way to learn Python?</td>\n",
       "      <td>Shares CS/tech topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110</td>\n",
       "      <td>Low</td>\n",
       "      <td>How do I build a mobile app?</td>\n",
       "      <td>Shares CS/tech topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   similarity relevance                               document  \\\n",
       "0       0.435    Medium             What is quantum computing?   \n",
       "1       0.319    Medium  What is the best way to learn Python?   \n",
       "2       0.110       Low           How do I build a mobile app?   \n",
       "\n",
       "                    why  \n",
       "0  Shares CS/tech topic  \n",
       "1  Shares CS/tech topic  \n",
       "2  Shares CS/tech topic  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def search_and_explain(query, top_n=5, min_sim=0.15):\n",
    "    results = semantic_search(query, documents, doc_embeddings, top_n=top_n)\n",
    "    rows = []\n",
    "    for doc, s in results:\n",
    "        label = \"High\" if s >= 0.45 else (\"Medium\" if s >= 0.25 else \"Low\")\n",
    "        rows.append({\n",
    "            \"similarity\": round(s, 3),\n",
    "            \"relevance\": label,\n",
    "            \"document\": doc,\n",
    "            \"why\": \"Shares CS/tech topic\" if any(w in doc.lower() for w in [\"python\",\"server\",\"app\",\"computing\",\"program\"]) else \"General\"\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df[df[\"similarity\"] >= min_sim].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Example\n",
    "search_and_explain(\"Explain programming languages.\", top_n=5, min_sim=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
